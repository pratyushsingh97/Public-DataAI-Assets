{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for testing performance of intent classification in Watson Assistant Service\n",
    "**[IBM Cloud](https://cloud.ibm.com/)** is a platform that hosts the **[Watson cognitive services](https://www.ibm.com/watson/developer/)** that leverage machine learning techniques to help partners and clients solve a variety business problems. Furthermore, several of the WDC services fall under the **supervised learning** suite of machine learning algorithms, that is, algorithms that learn by example. This begs the questions: \"How many examples should we provide?\" and \"When is my solution ready for prime time?\"\n",
    "\n",
    "It is critical to understand that training a machine learning solution is an iterative process where it is important to continually improve the solution by providing new examples and measuring the performance of the trained solution. In this notebook, we show how you can compute important Machine Learning metrics (accuracy, precision, recall, confusion_matrix) to judge the performance of your Watson Conversation service solution. For more details on these various metrics, please consult the **[Is Your Chatbot Ready for Prime-Time?](https://developer.ibm.com/dwblog/2016/chatbot-cognitive-performance-metrics-accuracy-precision-recall-confusion-matrix/)** blog post.\n",
    "\n",
    "You may also want to view some advice on best practices for Watson Assistant application development.  You can find these in **[Conversational Assistants and Quality with Watson Assistant - Revisted](https://medium.com/@dtoczala/conversational-assistants-and-quality-with-watson-assistant-revisited-123fb3bb9f1f)**.  We also really liked **[How to Design the Training Data for an AI Assitant](https://medium.com/ibm-watson/announcing-dialog-skill-analysis-for-watson-assistant-83cdfb968178)** and have imported a lot of their utility code (because we really like it).  We still use their Python packages throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:  \n",
    "This notebook was initially based upon a Python notebook provided by Joe Kozhaya, in his Watson Developer Cloud project on Github called **[wdcutils](https://github.com/joe4k/wdcutils/)**.  We also have \"borrowed\" some code from the excellent work shared in this blog post on **[How to Design the Training Data for an AI Assitant](https://medium.com/ibm-watson/announcing-dialog-skill-analysis-for-watson-assistant-83cdfb968178)**.  \n",
    "\n",
    "Further work to bring the code \"up to date\", and to extend some of the concepts, was done by the folowing contributors:\n",
    "- D. Toczala (dtoczala@us.ibm.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook assumes you have already created a **[Watson Assistant](https://cloud.ibm.com/catalog/services/watson-assistant)** service instance and trained a skill within it based on a number of intents. You will also need a Cloud Object Storage (COS) in stance with a single bucket for storing results of the k-fold testing.</br>\n",
    "<br> To leverage this notebook, you need to provide the following information:</br>\n",
    "\n",
    "* Credentials for your Watson Conversation instance \n",
    "* Credentials for your Cloud Object Storage (COS)\n",
    "\n",
    "If you do not wish to use COS for storing of results, just set the \"USE_COS\" flag in section 0.2 (Credentials) to \"False\".  Then you will not need to provide COS credential information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "0. [Part 0: Prepare](#part0)<br>\n",
    "    0.1 [Preparation](#part0.1)<br>\n",
    "    0.2 [Credentials](#part0.2)<br>\n",
    "    0.3 [Setup Cloud Object Storage (COS)](#part0.3)<br>\n",
    "    0.4 [Set Constants and Configuration Variables](#part0.4)<br>\n",
    "    0.5 [Declare Watson Assistant V1 routines](#part0.5)<br>\n",
    "1. [Part 1: Process Training Data](#part1)<br>\n",
    "    1.1 [Get current workspace and an Assistant instance](#part1.1)<br>\n",
    "    1.2 [Basic Utterance, Intent, Entity Processing](#part1.2)<br>\n",
    "    1.3 [Analyze Class Imbalance](#part1.3)<br>\n",
    "    1.4 [Distribution of Training Data](#part1.4)<br>\n",
    "    1.5 [Perform Correlation Analysis](#part1.5)<br>\n",
    "    1.6 [Visualize Terms Using a Heat Map](#part1.6)<br>\n",
    "    1.7 [Ambiguity in the Training Data](#part1.7)<br>\n",
    "2. [Part 2: Process Testing Data](#part2)<br>\n",
    "    2.1 [Break Data Into 5 K-fold Training/Testing Sets](#part2.1)<br>\n",
    "    2.2 [Loop Through Each Test \"Fold\"](#part2.2)<br>\n",
    "    2.3 [Build Confusion Matrix](#part2.3)<br>\n",
    "    2.4 [Analyze the Errors](#part2.4)<br>\n",
    "3. [Part 3: Advanced Analysis](#part3)<br>\n",
    "    3.1 [Perform Analysis Using Confidence Thresholds](#part3.1)<br>\n",
    "    3.2 [Analysis Interpretation at Confidence Level T](#part3.2)<br>\n",
    "    3.3 [Analyze Abnormal Confidence Levels](#part3.3)<br>\n",
    "    3.4 [Perform an Analysis Using Correlated Entities per Intent](#part3.4)<br>\n",
    "    3.5 [Calculate Base Chatbot Scores](#part3.5)<br>\n",
    "4. [Part 4: Summary](#part4)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part0'></a>\n",
    "# Part 0: Prepare\n",
    "0.1 [Preparation](#part0.1)<br>\n",
    "0.2 [Credentials](#part0.2)<br>\n",
    "0.3 [Setup Cloud Object Storage (COS)](#part0.3)<br>\n",
    "0.4 [Set Constants and Configuration Variables](#part0.4)<br>\n",
    "0.5 [Declare Watson Assistant V1 routines](#part0.5)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part0.1'></a>\n",
    "## 0.1 - Preparation\n",
    "\n",
    "Install and bring in all of the packages and code that you will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any warnings\n",
    "from IPython.display import Markdown, display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#\n",
    "# Go and download the non-standard packages that you need\n",
    "# Note: We use the assistant-dialog-skill-analysis package - but if they go in a different direction\n",
    "#       we might need to fork and just use the code that we need.\n",
    "#\n",
    "!pip install --index-url https://pypi.python.org/simple  -U \"pip\"\n",
    "!pip install --index-url https://pypi.python.org/simple  -U \"assistant-dialog-skill-analysis>=1.1.0\"\n",
    "!pip install ibm-watson\n",
    "!pip install pandas_ml\n",
    "#\n",
    "# We used to use pandas_ml for some processing - but it got to be out of date\n",
    "#\n",
    "#!pip install pandas_ml\n",
    "#\n",
    "# Standard Python libraries\n",
    "#\n",
    "import sys, os\n",
    "import json\n",
    "import importlib\n",
    "import types\n",
    "from collections import Counter\n",
    "import codecs\n",
    "import re\n",
    "import time\n",
    "from os.path import join, dirname\n",
    "from datetime import datetime\n",
    "import unicodecsv as csv\n",
    "\n",
    "#\n",
    "# Grab your data manipulation libraries\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "#\n",
    "# grab your display helpers\n",
    "#\n",
    "import seaborn as sn\n",
    "\n",
    "#\n",
    "# We used to use pandas_ml for some processing - but it got to be out of date\n",
    "# sklearn and matplotlib were also used by the original version - but are no longer needed\n",
    "#\n",
    "#import pandas_ml\n",
    "#from pandas_ml import ConfusionMatrix\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import precision_recall_fscore_support\n",
    "#from sklearn.metrics import classification_report\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "# IBM Watson and IBM Cloud libraries and SDKs\n",
    "#\n",
    "import ibm_watson\n",
    "from ibm_watson import ApiException\n",
    "from ibm_watson import AssistantV1\n",
    "from ibm_watson import AssistantV2\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "#\n",
    "# Grab that Watson Assistant Skill Analysis library\n",
    "#\n",
    "from assistant_dialog_skill_analysis.utils import skills_util\n",
    "from assistant_dialog_skill_analysis.highlighting import highlighter\n",
    "from assistant_dialog_skill_analysis.data_analysis import summary_generator\n",
    "from assistant_dialog_skill_analysis.data_analysis import divergence_analyzer\n",
    "from assistant_dialog_skill_analysis.data_analysis import similarity_analyzer\n",
    "from assistant_dialog_skill_analysis.term_analysis import chi2_analyzer\n",
    "from assistant_dialog_skill_analysis.term_analysis import keyword_analyzer\n",
    "from assistant_dialog_skill_analysis.term_analysis import entity_analyzer\n",
    "from assistant_dialog_skill_analysis.confidence_analysis import confidence_analyzer\n",
    "from assistant_dialog_skill_analysis.inferencing import inferencer\n",
    "from assistant_dialog_skill_analysis.experimentation import data_manipulator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part0.2'></a>\n",
    "## 0.2 - Credentials\n",
    "Provide the credentials to access your Conversation service and a COS storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "#\n",
    "# Cloud Object Storage (COS) and IAM Parameters\n",
    "#\n",
    "#    'USE_COS' : can be either 'True', if using COS to store results, or 'False' if not using COS\n",
    "#    'IAM_SERVICE_ID': the IAM service ID - you can find this on the \"Service Credentials\" tab, just\n",
    "#                      select the service key and click on the \"View Credentials\" caret, and in the\n",
    "#                      JSON that is shown, choose the \"iam_serviceid_crn\" field,\n",
    "#    'IBM_API_KEY_ID': the IAM service ID - you can find this on the \"Service Credentials\" tab, just\n",
    "#                      select the service key and click on the \"View Credentials\" caret, and in the\n",
    "#                      JSON that is shown, choose the \"apikey\" field,\n",
    "#    'ENDPOINT': your API endpoint.  Check the endpoints specified in COS, for your specific region, \n",
    "#                which can be found at https://cloud.ibm.com/docs/services/cloud-object-storage?topic=cloud-object-storage-endpoints\n",
    "#    'IBM_AUTH_ENDPOINT': the IBM Authentiocation endpoint - https://iam.bluemix.net/oidc/token,\n",
    "#    'RESULTS_BUCKET': Plain text name of the COS bucket for your results,\n",
    "#    'BUCKET': Plain text name of the COS bucket you are accessing for credential information\n",
    "#\n",
    "    'USE_COS' : 'True',\n",
    "    'IAM_SERVICE_ID': 'crn:v1:bluemix:public:iam-identity::a/d50fb47sjewy36597f8f2efdwe53eeds5::serviceid:ServiceId-2beye65f-2ww9-4226-beef-7363eds5233a',\n",
    "    'IBM_API_KEY_ID': '84823hyrTH62UY762dlf7wmrPkZrh12eytey346dejqw7HW5N',\n",
    "    'ENDPOINT': 'https://s3.us-south.cloud-object-storage.appdomain.cloud',\n",
    "    'IBM_AUTH_ENDPOINT': 'https://iam.bluemix.net/oidc/token',\n",
    "    'BUCKET': 'my-bucket',\n",
    "#\n",
    "# Assistant creds\n",
    "#\n",
    "#    'ASSISTANT_API_KEY': the API key for your Watson Assistant instance,\n",
    "#    'ASSISTANT_VERSION': the version of the Watson Assistant API being used, generally something like '2019-02-28',\n",
    "#    'ASSISTANT_URL': path to the service URL, generally this is 'https://gateway.watsonplatform.net/assistant/api',\n",
    "#    'ASSISTANT_SKILL_ID': ID of the skill being worked on and tested\n",
    "#\n",
    "    'ASSISTANT_API_KEY': 'xxx999999999863Eshsyetd3GFotx4pBtoxtoxwesd6x',\n",
    "    'ASSISTANT_VERSION': '2019-02-28',\n",
    "    'ASSISTANT_URL': 'https://gateway.watsonplatform.net/assistant/api',\n",
    "    'ASSISTANT_SKILL_ID': '2354west-45e4-4a23-8ac1-e6187ert5w45'\n",
    "#\n",
    "# Note that the ASSISTANT SKILL ID is tricky to get.\n",
    "#\n",
    "# In order to get this, go into Watson Assistant and look at the Skills viewer.  For the skill that you want to use, \n",
    "# click on the three dots in the upper right corner, and select \"View API Details\".  On the ensuing page, look at the\n",
    "# credentials at the top of the page, and find the one that looks like this:\n",
    "#\n",
    "# Legacy v1 Workspace URL:https://gateway.watsonplatform.net/assistant/api/v1/workspaces/2354west-45e4-4a23-8ac1-e6187ert5w45/message\n",
    "#\n",
    "# Note that the ASSISTANT SKILL ID is actually the V1 Workspace ID, which is the second to last part of the path in \n",
    "# the example above (2354west-45e4-4a23-8ac1-e6187ert5w45).  This is the unique identifier for the workspace.\n",
    "}\n",
    "\n",
    "#######################################################\n",
    "#\n",
    "# CONSTANTS \n",
    "#\n",
    "MAX_TERMS_DISPLAY = 30                # Total number of terms to display in various outputs\n",
    "#\n",
    "INTENTS_TO_DISPLAY = 30               # Total number of intents for display\n",
    "#\n",
    "USE_COS = credentials['USE_COS']      # Will you be using Cloud Object Storage to store results?\n",
    "#\n",
    "THREAD_NUM = 3                        # Maximum number of concurrent threads to use at any one time\n",
    "#\n",
    "#######################################################\n",
    "#\n",
    "# VARIABLES\n",
    "#\n",
    "# These variables and lists should be user entered and will differ for each individual Watson Assistant instance.\n",
    "# As you use this to test your model over time, you will want to focus analysis on certain problem areas.  These\n",
    "# variables and list highlight those areas to focus on.\n",
    "#\n",
    "# Later in the code we have a way to analyze specific intents in more depth.\n",
    "# Add the names of those intents to this list to force that analysis.\n",
    "#\n",
    "# For example: focused_intent_list = [greeting, transaction, customer_support]\n",
    "#\n",
    "focused_intent_list = []\n",
    "#\n",
    "# Later in the code we have a way to compare specific intents in more depth for overlap.\n",
    "# Add the names of those intents to this list to force that analysis.\n",
    "#\n",
    "# For example: \n",
    "#    overlap_intent_1 = 'transaction'\n",
    "#    overlap_intent_2 = 'customer_support'\n",
    "#\n",
    "overlap_intent_1 = ''\n",
    "overlap_intent_2 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "\n",
    "credentials = {\n",
    "    'IAM_SERVICE_ID': 'crn:v1:bluemix:public:iam-identity::a/d50fb4700dcb8797f8f2efd18a1d22f5::serviceid:ServiceId-2bbc061f-2ad9-4ac6-bfcd-7321e5ecd83a',\n",
    "    'IBM_API_KEY_ID': '8d0GW7wfsmLCuf6ozXzcwmrPkZrh6bV8B4cvXNfwmY7N',\n",
    "    'ENDPOINT': 'https://s3.us-south.cloud-object-storage.appdomain.cloud',\n",
    "    'IBM_AUTH_ENDPOINT': 'https://iam.bluemix.net/oidc/token',\n",
    "    'BUCKET': 'csm-bot-bucket',\n",
    "#\n",
    "# Assistant creds\n",
    "#\n",
    "    # The Dev instance\n",
    "    'ASSISTANT_API_KEY': '1UsR8El_eTxEtgTvRd7-rRZ1qwx7d8TcdH8Rw4URWbi3',  # \"writer\" Assistant API key \n",
    "    'ASSISTANT_VERSION': '2019-02-28',\n",
    "    'ASSISTANT_URL': 'https://gateway.watsonplatform.net/assistant/api',\n",
    "    'ASSISTANT_SKILL_ID': 'de65f0c8-29f6-4612-9969-2ec940c06609'   # using Skill ID\n",
    "\n",
    "    # the \"Regions\" instance in Test area\n",
    "#    'ASSISTANT_API_KEY': 'jPOesPLpJLhnqFHwZ7axHOfKGYT3AZuSgQ45pbFHo2BV',  # \"writer\" Assistant API key \n",
    "#    'ASSISTANT_VERSION': '2019-02-28',\n",
    "#    'ASSISTANT_URL': 'https://gateway.watsonplatform.net/assistant/api',\n",
    "#    'ASSISTANT_SKILL_ID': '5fd5f305-11e5-4b9e-928e-a19a5dec24a6'   # using Skill ID\n",
    "\n",
    "}\n",
    "#\n",
    "# Null setup\n",
    "#\n",
    "#focused_intent_list = []\n",
    "#\n",
    "#overlap_intent_1 = ''\n",
    "#overlap_intent_2 = ''\n",
    "\n",
    "#\n",
    "# CSM Bot intents to focus on\n",
    "#\n",
    "focused_intent_list = ['site_help', 'plan_info']\n",
    "#\n",
    "overlap_intent_1 = 'site_help'\n",
    "overlap_intent_2 = 'plan_info'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part0.3'></a>\n",
    "## 0.3 - Setup Cloud Object Storage (COS)\n",
    "\n",
    "Create your COS object and get it all setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "DEBUG=False\n",
    "#DEBUG=True\n",
    "#\n",
    "# Grab creds file from COS storage\n",
    "#\n",
    "# IBM COS interface\n",
    "if USE_COS:\n",
    "    def __iter__(self): return 0\n",
    "    from ibm_botocore.client import Config\n",
    "\n",
    "    cos = ibm_boto3.client(service_name='s3',\n",
    "        ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
    "        ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
    "        ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
    "        config=Config(signature_version='oauth'),\n",
    "        endpoint_url=credentials['ENDPOINT'])\n",
    "#\n",
    "# Build filenames that you will use\n",
    "#\n",
    "UNDERSCORE = '_'\n",
    "#\n",
    "# First get current date/time\n",
    "#\n",
    "myDatetime = datetime.now()\n",
    "if (DEBUG):\n",
    "    print (myDatetime)\n",
    "myDatetime = re.sub(r'\\s',UNDERSCORE,str(myDatetime))\n",
    "goodDatetime,junk = myDatetime.split('.')\n",
    "#\n",
    "# Build filenames\n",
    "#\n",
    "# Watson Assistant workspace definition (model)\n",
    "#\n",
    "Workspace_file = 'Backup_Assistant_Workspace_'+goodDatetime+'.json'\n",
    "Workspace_path = './'+ Workspace_file\n",
    "#\n",
    "# K-Fold test results\n",
    "#\n",
    "TestResults_file = 'KFold_Results_'+goodDatetime+'.csv'\n",
    "TestResults_path = './'+ TestResults_file\n",
    "#\n",
    "# K-Fold test results\n",
    "#\n",
    "Scores_file = 'KFold_Scores_'+goodDatetime+'.csv'\n",
    "Scores_path = './'+ Scores_file\n",
    "#\n",
    "# Confusion Matrix\n",
    "#\n",
    "ConfMatrix_file = 'Confusion_Matrix_'+goodDatetime+'.csv'\n",
    "ConfMatrix_path = './'+ ConfMatrix_file\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part0.4'></a>\n",
    "## 0.4 - Set Constants and Configuration Variables\n",
    "\n",
    "Set variables for those credentials and for accessing Watson services, COS buckets, and other resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Set all required parameters\n",
    "# \n",
    "assist_api_key=credentials['ASSISTANT_API_KEY']\n",
    "wa_version=credentials['ASSISTANT_VERSION']\n",
    "wa_url=credentials['ASSISTANT_URL']\n",
    "assist_workspace_id=credentials['ASSISTANT_SKILL_ID']\n",
    "#\n",
    "# Change URL based on IBM Cloud datacenter you use \n",
    "#\n",
    "URL = credentials['ASSISTANT_URL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part0.5'></a>\n",
    "## 0.5 - Declare Watson Assistant V1 routines\n",
    "Define useful methods to classify using trained Watson Assistant service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Given a pointer to an Assistamt instance and a workspace ID, go and delete a workspace, get back status\n",
    "#\n",
    "def deleteAssistantWorkspace(assist_instance,workspaceID):\n",
    "    context={}\n",
    "    MAX_ATTEMPTS = 2\n",
    "    tries = 0\n",
    "    need_results = True\n",
    "    while (need_results and (tries < MAX_ATTEMPTS)):\n",
    "        try:\n",
    "            response = assist_instance.delete_workspace(\n",
    "                workspace_id=workspaceID)\n",
    "            need_results = False\n",
    "        except Exception as e:\n",
    "            print(\"will retry after error\", e)\n",
    "            tries += 1\n",
    "            time.sleep(5)\n",
    "        # end of loop to retry\n",
    "\n",
    "    if tries < MAX_ATTEMPTS:\n",
    "        workspaceStatus=response.get_result()\n",
    "        \n",
    "    return workspaceStatus\n",
    "\n",
    "#\n",
    "# Given a pointer to an Assistamt instance and workspaceID, get back Assistantkeep checking the workspace\n",
    "# and don't return until workspace is \"Available\" - return true if workspace is available, false if timeout\n",
    "# reached \n",
    "#\n",
    "def waitAssistantWorkspace(assist_instance,workspaceID,timeout):\n",
    "    stat=False\n",
    "    MAX_ATTEMPTS = int (timeout // 5)\n",
    "    tries = 0\n",
    "    while (stat == False) and (tries < MAX_ATTEMPTS):\n",
    "        try:\n",
    "            response = assist_instance.get_workspace(\n",
    "                workspace_id=workspaceID)\n",
    "        except:\n",
    "            tries += 1\n",
    "            time.sleep(5)\n",
    "            sys.stdout.write('.')\n",
    "            continue\n",
    "        #\n",
    "        # Now check the status\n",
    "        #\n",
    "        settings = response.get_result()\n",
    "        if (settings['status'] == \"Available\"):\n",
    "            stat=True\n",
    "            break\n",
    "        else:\n",
    "            tries += 1\n",
    "            time.sleep(5)\n",
    "            sys.stdout.write('.')\n",
    "        # end of loop to retry \n",
    "    return stat\n",
    "\n",
    "#\n",
    "# Taking a Watson Assistant workspace JSON object, break the intents and training data\n",
    "# into a series of \"k-folds\", based on the percentages given.  So for a k=5 k-fold test, \n",
    "# we would call this 5 times, with start/end values of 0/20, 20/40, 40/60, 60/80, and 80/100.\n",
    "# For a k=3 k-fold test, we would call this 3 times, with start/end values of 0/33, 34/66, \n",
    "# and 67/100.\n",
    "# \n",
    "\n",
    "def kfold_sampling(workspace, slice_start_pct=0.0, slice_end_pct=0.2):\n",
    "    \"\"\"\n",
    "    Create a stratified sample of the workspace json\n",
    "    & return a intent json acceptable in Assistant API\n",
    "    :param workspace: json format output defined by Assistant API\n",
    "    :param sampling_percentage: percentage of original to sample\n",
    "    :return train_workspace_data: list of intents for train\n",
    "    :return test_workspace_data: list of utterance,intent pairs for test\n",
    "    \"\"\"\n",
    "    train_workspace_data = list()\n",
    "    test_workspace_data = list()\n",
    "    for i in range(len(workspace[\"intents\"])):\n",
    "        intent = workspace[\"intents\"][i]\n",
    "        sampling_index = list(np.arange(len(intent[\"examples\"])))\n",
    "        # training set\n",
    "        train_test_split_start = int(slice_start_pct * len(sampling_index))\n",
    "        train_test_split_cutoff = int(slice_end_pct * len(sampling_index))\n",
    "        train_examples_1 = [\n",
    "            intent[\"examples\"][index]\n",
    "            for index in sampling_index[0:train_test_split_start]\n",
    "        ]\n",
    "        train_examples_2 = [\n",
    "            intent[\"examples\"][index]\n",
    "            for index in sampling_index[train_test_split_cutoff:]\n",
    "        ]\n",
    "        train_examples = train_examples_1 + train_examples_2\n",
    "        train_workspace_data.append({\"intent\": workspace[\"intents\"][i][\"intent\"]})\n",
    "        train_workspace_data[i].update({\"description\": \"string\"})\n",
    "        train_workspace_data[i].update({\"examples\": train_examples})\n",
    "        # test set\n",
    "        test_examples = [\n",
    "            intent[\"examples\"][index]\n",
    "            for index in sampling_index[train_test_split_start:train_test_split_cutoff]\n",
    "        ]\n",
    "        #\n",
    "        test_example_data = [\n",
    "            {'utterance': utterances[\"text\"], 'intent': workspace[\"intents\"][i][\"intent\"]}\n",
    "            for utterances in test_examples\n",
    "            ]\n",
    "        test_workspace_data.extend(test_example_data)\n",
    "        #\n",
    "        # Convert to a data frame\n",
    "        #\n",
    "        test_workspace_df = pd.DataFrame(test_workspace_data)\n",
    "        \n",
    "    return train_workspace_data, test_workspace_df\n",
    "\n",
    "#\n",
    "# Taking a Watson Assistant object, create a new workspace and apply the training data given.\n",
    "# Then apply the test set provided, and collect results.  Delete the workspace, and return\n",
    "# the test results.\n",
    "# \n",
    "def processKFold (assist_instance,training_json,test_df):\n",
    "    #\n",
    "    # Create workspace, and save workspace ID\n",
    "    #\n",
    "    print(\"Begin test run \")\n",
    "    testAssistWorkspace=skills_util.create_workspace(assist_instance, intent_json=training_json)\n",
    "    testWorkspaceID = testAssistWorkspace[\"workspace_id\"]\n",
    "    #\n",
    "    # Wait for training to complete\n",
    "    #\n",
    "    # print(\"Created first fold test environment \")\n",
    "    stat = waitAssistantWorkspace(assist_instance,testWorkspaceID,600) # 600 second timeout for training (ten minutes)\n",
    "    #\n",
    "    # Run your tests - note we suggest a maximum of 5 threads for faster inference\n",
    "    #\n",
    "    full_results = inferencer.inference(assist_instance,\n",
    "                                        testWorkspaceID,\n",
    "                                        test_df,\n",
    "                                        max_retries=10,\n",
    "                                        max_thread=THREAD_NUM, \n",
    "                                        verbose=False)\n",
    "    #\n",
    "    # Delete the test workspace\n",
    "    #\n",
    "    stat = deleteAssistantWorkspace(assist_instance,testWorkspaceID)\n",
    "    #\n",
    "    return full_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "# Part 1: Process Training Data\n",
    "1.1 [Get current workspace and an Assistant instance](#part1.1)<br>\n",
    "1.2 [Basic Utterance, Intent, Entity Processing](#part1.2)<br>\n",
    "1.3 [Analyze Class Imbalance](#part1.3)<br>\n",
    "1.4 [Distribution of Training Data](#part1.4)<br>\n",
    "1.5 [Perform Correlation Analysis](#part1.5)<br>\n",
    "1.6 [Visualize Terms Using a Heat Map](#part1.6)<br>\n",
    "1.7 [Ambiguity in the Training Data](#part1.7)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1.1'></a>\n",
    "## 1.1 - Get current workspace and an Assistant instance\n",
    "\n",
    "We want to get a copy of the current workspace, which we will manipulate for testing purposes, as well as an object for the Watson Assistant instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(skills_util)\n",
    "#\n",
    "#\n",
    "#\n",
    "conversation, workspace = skills_util.retrieve_workspace(iam_apikey=assist_api_key,\n",
    "                                                         workspace_id=assist_workspace_id,\n",
    "                                                         url=URL,\n",
    "                                                         api_version=wa_version)\n",
    "\n",
    "#\n",
    "# If COS is enabled, save your Workspace off\n",
    "#\n",
    "if USE_COS:\n",
    "    with open(Workspace_file, 'w') as fp:\n",
    "        json.dump(workspace, fp)\n",
    "    fp.close()\n",
    "    #\n",
    "    # Write Assistant settings out to COS\n",
    "    #\n",
    "    cos.upload_file(Filename=Workspace_path,Bucket=credentials['BUCKET'],Key=Workspace_file)\n",
    "    #\n",
    "    print (\"Workspace saved off to file \" + Workspace_file + \" in results area.\")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1.2'></a>\n",
    "## 1.2 Basic Utterance, Intent, Entity Processing\n",
    "\n",
    "Generate summary statistics related to the given skill and workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Extract user workspace data and vocabulary\n",
    "#\n",
    "workspace_data, workspace_vocabulary = skills_util.extract_workspace_data(workspace)\n",
    "#\n",
    "# Build a dictionary of entities and an entity list\n",
    "#\n",
    "entity_dict = conversation.list_entities(assist_workspace_id).get_result()\n",
    "entities_list = [item['entity'] for item in entity_dict['entities']]\n",
    "#\n",
    "# Create a workspace data frame from Workspace data\n",
    "#\n",
    "workspace_pd = pd.DataFrame(workspace_data)\n",
    "#\n",
    "# Dump sample (first 10) of utterances and intents\n",
    "#\n",
    "display(Markdown(\"### Sample of Utterances & Intents\"))\n",
    "display(HTML(workspace_pd.sample(n = len(workspace_pd) if len(workspace_pd)<10 else 10)\n",
    "             .to_html(index=False)))\n",
    "#\n",
    "# Dump sample (first 10) of entities, if there are any\n",
    "#\n",
    "if entities_list:\n",
    "    display(Markdown(\"### Sample of Entities\"))\n",
    "    display(HTML(pd.DataFrame({\"Entity\":entities_list})\n",
    "                 .sample(n = len(entities_list) if len(entities_list)<10 else 10)\n",
    "                 .to_html(index=False)))\n",
    "#\n",
    "# Dump summary data\n",
    "#\n",
    "importlib.reload(summary_generator)\n",
    "summary_generator.generate_summary_statistics(workspace_data, entities_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1.3'></a>\n",
    "## 1.3 - Analyze Class Imbalance\n",
    "\n",
    "Analyze whether the data set contains class imbalance by checking whether the largest intent contains less than double the number of user examples contained in the smallest intent. If there is an imbalance it does not necessarily indicate an issue; but you should review the [actions](#actionimbalance) section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(summary_generator)\n",
    "class_imb_flag = summary_generator.class_imbalance_analysis(workspace_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions for class imbalance<a id='actionimbalance'></a>\n",
    "\n",
    "Class imbalance will not always lead to lower accuracy, which means that all intents (classes) do not need to have the same number of examples.\n",
    "\n",
    "Given a hypothetical chatbot related to banking:<br>\n",
    "\n",
    "- For intents like `updateBankAccount` and `addNewAccountHolder` where the semantics difference between them is subtler, the number of examples per intent needs to be somewhat balanced otherwise the classifier might favor the intent with the higher number of examples.\n",
    "- For intents like `greetings` that are semantically distinct from other intents like `updateBankAccount`, it may be acceptable for it to have fewer examples per intent and still be easy for the intent detector to classify.\n",
    "\n",
    "If the intent classification accuracy is lower than expected during testing, you should re-examine the distribution analysis.  \n",
    "\n",
    "With regard to sorted distribution of examples per intent, if the sorted number of user examples varies a lot across different intents, it can be a potential source of bias for intent detection. Large imbalances in general should be avoided. This can potentially lead to lower accuracy. If your graph displays this characteristic, this could be a source of error.\n",
    "\n",
    "For further guidance on adding more examples to help balance out your distribution, refer to \n",
    "<a href=\"https://cloud.ibm.com/docs/services/assistant?topic=assistant-intent-recommendations#intent-recommendations-get-example-recommendations\" target=\"_blank\" rel=\"noopener no referrer\">Intent Example Recommendation</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1.4'></a>\n",
    "## 1.4 - Distribution of Training Data\n",
    "Display the distribution of intents versus the number of examples per intent (sorted by the number of examples per intent) below. Ideally you should not have large variations in terms of number of user examples for various intents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build and display plot of intent distribution\n",
    "#\n",
    "#importlib.reload(summary_generator)\n",
    "#summary_generator.scatter_plot_intent_dist(workspace_pd)\n",
    "#\n",
    "# Build and display list of intent distribution\n",
    "#\n",
    "importlib.reload(summary_generator)\n",
    "summary_generator.show_user_examples_per_intent(workspace_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You MUST HAVE TEN OR MORE EXAMPLES for each intent!!\n",
    "\n",
    "We are going to do a k-fold analysis of the training data, and in order for this to work correctly and the data to be accurate, we need to have at least two examples in each \"fold\" of the k-fold groups. Since we are doing this with k=5 (five folds of your data), you need to <b>have at least 10 examples for each intent</b>.\n",
    "\n",
    "It's also considered a <b>Watson Assistant Best Practice</b> to have at least 10 utterances/examples for each intent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1.5'></a>\n",
    "## 1.5 - Perform Correlation Analysis\n",
    "\n",
    "- [Retrieve the most correlated unigrams and bigrams for each intent](#retrieve)\n",
    "- [Actions for anomalous correlations](#anomalous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the most correlated unigrams and bigrams for each intent<a id='retrieve'></a>\n",
    "\n",
    "Perform a chi square significance test using count features to determine the terms that are most correlated with each intent in the data set. \n",
    "\n",
    "A `unigram` is a single word, while a `bigram` is two consecutive words from within the training data. For example, if you have a sentence like `Thank you for your service`, each of the words in the sentence are considered unigrams while terms like `Thank you`, `your service` are considered bigrams.\n",
    "\n",
    "Terms such as `hi`, `hello` correlated with a `greeting` intent are reasonable. But terms such as `table`, `chair` correlated with the `greeting` intent are anomalous. A scan of the most correlated unigrams & bigrams for each intent can help you spot potential anomalies within your training data.\n",
    "\n",
    "**Note**: We ignore the following common words (\\\"stop words\\\") from consideration `an, a, in, on, be, or, of, a, and, can, is, to, the, i`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(chi2_analyzer)\n",
    "unigram_intent_dict, bigram_intent_dict = chi2_analyzer.get_chi2_analysis(workspace_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions for anomalous correlations<a id='anomalous'></a>\n",
    "\n",
    "If you identify unusual or anomalous correlated terms such as: numbers, names and so on, which should not be correlated with an intent, consider the following:\n",
    "  \n",
    "- **Case 1** : If you see names appearing amongst correlated unigrams or bigrams, add more variation of names so no specific names will be correlated  \n",
    "- **Case 2** : If you see specific numbers like 1234 amongst correlated unigrams or bigrams and these are not helpful to the use case, remove or mask these numbers from the examples\n",
    "- **Case 3** : If you see terms which should never be correlated to that specific intent, consider adding or removing terms/examples so that domain specific terms are correlated with the correct intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1.6'></a>\n",
    "## 1.6 - Visualize Terms Using a Heat Map\n",
    "\n",
    "- [Display term analysis for a custom intent list](#customintent)\n",
    "- [Actions for anomalous terms in the heat map](#heatmap)\n",
    "\n",
    "A heat map of terms is a method to visualize terms or words that frequently occur within each intent. Rows are the terms, and columns are the intents. \n",
    "\n",
    "The code below displays the top 30 intents with the highest number of user examples in the analysis. This number can be changed if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(keyword_analyzer)\n",
    "intent_list = []\n",
    "keyword_analyzer.seaborn_heatmap(workspace_pd, INTENTS_TO_DISPLAY, MAX_TERMS_DISPLAY, intent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display term analysis for a custom intent list<a id='customintent'></a>\n",
    "\n",
    "If you wish to see term analysis for specific intents, feel free to add those intents to the intent list. This generates a custom term heatmap. The code below displays the top 20 terms, but this can be changed if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(keyword_analyzer)\n",
    "# focused_intent_list = ['intent_a', 'intent_b']   # set in section 0.2\n",
    "intent_list = focused_intent_list\n",
    "\n",
    "if intent_list: \n",
    "    keyword_analyzer.seaborn_heatmap(workspace_pd, INTENTS_TO_DISPLAY, MAX_TERMS_DISPLAY, intent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions for anomalous terms in the heat map<a id='heatmap'></a>\n",
    "\n",
    "If you notice any terms or words which should not be frequently present within an intent, consider modifying examples in that intent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1.7'></a>\n",
    "## 1.7 - Ambiguity in the Training Data\n",
    "\n",
    "- [Uncover ambiguous utterances across intents](#uncover)\n",
    "- [Actions for ambiguity in the training data](#ambiguityaction)\n",
    "\n",
    "Run the code blocks below to uncover possibly ambiguous terms based on feature correlation.\n",
    "\n",
    "Based on the chi-square analysis above, generate intent pairs which have overlapping correlated unigrams and bigrams.\n",
    "This allows you to get a glimpse of which unigrams or bigrams might cause potential confusion with intent detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Intent Pairs with Overlapping Correlated Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(chi2_analyzer)\n",
    "display(Markdown(\"### Top Intent Pairs with Overlapping Correlated Unigrams\"))\n",
    "ambiguous_unigram_df = chi2_analyzer.get_confusing_key_terms(unigram_intent_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Intent Pairs with Overlapping Correlated Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(chi2_analyzer)\n",
    "display(Markdown(\"### Top Intent Pairs with Overlapping Correlated Bigrams\"))\n",
    "ambiguous_bigram_df = chi2_analyzer.get_confusing_key_terms(bigram_intent_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlap Checker for Specific Intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add specific intent or intent pairs for which you would like to see overlap\n",
    "importlib.reload(chi2_analyzer)\n",
    "# overlap_intent_1 = 'intent1'  # this is set in section 0.2\n",
    "# overlap_intent_2 = 'intent2'  # this is set in section 0.2\n",
    "display(Markdown(\"### Potential Term Overlap in Intent Training Data\"))\n",
    "chi2_analyzer.chi2_overlap_check(ambiguous_unigram_df,ambiguous_bigram_df,overlap_intent_1,overlap_intent_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncover ambiguous utterances across intents<a id='uncover'></a>\n",
    "The following analysis shows user examples that are similar but fall under different intents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(similarity_analyzer)\n",
    "display(Markdown(\"### Ambiguous Utterances in Training Data\"))\n",
    "similar_utterance_diff_intent_pd = similarity_analyzer.ambiguous_examples_analysis(workspace_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions for ambiguity in the training data<a id='ambiguityaction'></a>\n",
    "\n",
    "**Ambiguous intent pairs**  \n",
    "If you see terms which are correlated with more than 1 intent, review if this seems anomalous based on the use case for that intent. If it seems reasonable, it is probably not an issue.  \n",
    "\n",
    "**Ambiguous utterances across intents** \n",
    "- **Duplicate utterances**: For duplicate or almost identical utterances, remove those that seem unnecessary.\n",
    "- **Similar utterances**: For similar utterances, review the use case for those intents and make sure that they are not accidental additions caused by human error when the training data was created.  \n",
    "\n",
    "For more information about entity, refer to the <a href=\"https://cloud.ibm.com/docs/services/assistant/services/assistant?topic=assistant-entities\" target=\"_blank\" rel=\"noopener no referrer\">Entity Documentation</a>.\n",
    "\n",
    "For more in-depth analysis related to possible conflicts in your training data across intents, try the conflict detection feature in Watson Assistant. Refer to <br> <a href=\"https://cloud.ibm.com/docs/services/assistant?topic=assistant-intents#intents-resolve-conflicts\" target=\"_blank\" rel=\"noopener no referrer\">Conflict Resolution Documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  This block is just a DEBUG block\n",
    "# I use it to help visualize what the various JSON, lists, and data frames look like\n",
    "#\n",
    "DEBUG=False\n",
    "#DEBUG=True\n",
    "\n",
    "if (DEBUG):\n",
    "    print (\"\\n\\n==========\\n workspace_pd \\n\")\n",
    "    print (workspace_pd)\n",
    "    #\n",
    "    tox_df = pd.DataFrame(workspace_data)\n",
    "    print (\"\\n\\n==========\\n tox_df \\n\")\n",
    "    print (tox_df)\n",
    "    #\n",
    "    tox_json = tox_df.to_json(orient='records')\n",
    "    print (\"\\n\\n==========\\n tox_json \\n\")\n",
    "    print (tox_json)\n",
    "    #\n",
    "    print (\"\\n\\n==========\\n workspace \\n\")\n",
    "    print (workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "# Part 2: Process Testing Data\n",
    "2.1 [Break Data Into 5 K-fold Training/Testing Sets](#part2.1)<br>\n",
    "2.2 [Loop Through Each Test \"Fold\"](#part2.2)<br>\n",
    "2.3 [Build Confusion Matrix](#part2.3)<br>\n",
    "2.4 [Analyze the Errors](#part2.4)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2.1'></a>\n",
    "## 2.1 - Break Data Into 5 K-fold Training/Testing Sets\n",
    "\n",
    "We want to take our copy of the current workspace, and th.en break it into 5 sets of training data and associated test data, each with a distribution of 20% of each intent for test data, and 80% of each intents for training data.  I could try to make this random, but I don't have the time today.  Instead, we will take the data for each intent, and break it into five pieces.  So you will see something like this:\n",
    "\n",
    "**Intent A** -> split into 5 pieces, A1, A2, A3, A4, and A5.\n",
    "**Intent B** -> split into 5 pieces: B1, B2, B3, B4, and B5.\n",
    "\n",
    "**Training set 1** -> A2 + A3 + A4 + A5 + B2 + B3 + B4 + B5\n",
    "**Test Set 1** -> A1 + B1\n",
    "\n",
    "**Training set 2** -> A1 + A3 + A4 + A5 + B1 + B3 + B4 + B5\n",
    "**Test set 2** -> A2 + B2\n",
    "\n",
    "**Training set 3** -> A1 + A2 + A4 + A5 + B1 + B2 + B4 + B5\n",
    "**Test set 3** -> A3 + B3\n",
    "\n",
    "**Training set 4** -> A1 + A2 + A3 + A5 + B1 + B2 + B3 + B5\n",
    "**Test set 4** -> A4 + B4\n",
    "\n",
    "**Training set 5** -> A1 + A2 + A3 + A4 + B1 + B2 + B3 + B4\n",
    "**Test set 5** -> A5 + B5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG=True\n",
    "DEBUG=False\n",
    "#\n",
    "(TrainSet1_json, TestSet1) = kfold_sampling(workspace,slice_start_pct=0.0, slice_end_pct=0.2) \n",
    "(TrainSet2_json, TestSet2) = kfold_sampling(workspace,slice_start_pct=0.2, slice_end_pct=0.4)\n",
    "(TrainSet3_json, TestSet3) = kfold_sampling(workspace,slice_start_pct=0.4, slice_end_pct=0.6)\n",
    "(TrainSet4_json, TestSet4) = kfold_sampling(workspace,slice_start_pct=0.6, slice_end_pct=0.8)\n",
    "(TrainSet5_json, TestSet5) = kfold_sampling(workspace,slice_start_pct=0.8, slice_end_pct=1.0)\n",
    "#\n",
    "# Now see how you split things up\n",
    "#\n",
    "if (DEBUG):\n",
    "    print('\\n========\\nTRAIN SET 1\\n')\n",
    "    print(TrainSet1_json)\n",
    "    print('\\n========\\nTEST SET 1\\n')\n",
    "    print(TestSet1)\n",
    "    #\n",
    "    print('\\n========\\nTRAIN SET 2\\n')\n",
    "    print(TrainSet2_json)\n",
    "    print('\\n========\\nTEST SET 2\\n')\n",
    "    print(TestSet2)\n",
    "    #\n",
    "    print('\\n========\\nTRAIN SET 3\\n')\n",
    "    print(TrainSet3_json)\n",
    "    print('\\n========\\nTEST SET 3\\n')\n",
    "    print(TestSet3)\n",
    "    #\n",
    "    print('\\n========\\nTRAIN SET 4\\n')\n",
    "    print(TrainSet4_json)\n",
    "    print('\\n========\\nTEST SET 4\\n')\n",
    "    print(TestSet4)\n",
    "    #\n",
    "    print('\\n========\\nTRAIN SET 5\\n')\n",
    "    print(TrainSet5_json)\n",
    "    print('\\n========\\nTEST SET 5\\n')\n",
    "    print(TestSet5)\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2.2'></a>\n",
    "# 2.2 Loop Through Each Test \"Fold\"\n",
    "This is hardcoded to do k-fold testing with a K value = 5 right now.\n",
    "\n",
    "Steps done are as follows:\n",
    "1. Create a test workspace and load in training data\n",
    "2. Run tests with test data against test workspace\n",
    "3. Collect results\n",
    "4. Delete test workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#DEBUG = True\n",
    "DEBUG = False\n",
    "#\n",
    "# Process each fold\n",
    "#\n",
    "fold_1_results = processKFold (conversation,TrainSet1_json,TestSet1)\n",
    "fold_2_results = processKFold (conversation,TrainSet2_json,TestSet2)\n",
    "fold_3_results = processKFold (conversation,TrainSet3_json,TestSet3)\n",
    "fold_4_results = processKFold (conversation,TrainSet4_json,TestSet4)\n",
    "fold_5_results = processKFold (conversation,TrainSet5_json,TestSet5)\n",
    "#\n",
    "frames = [fold_1_results, fold_2_results, fold_3_results, fold_4_results, fold_5_results]\n",
    "full_results = pd.concat(frames)\n",
    "#\n",
    "# Dump results to COS file if COS is enabled\n",
    "#\n",
    "#\n",
    "# If COS is enabled, save your Workspace off\n",
    "#\n",
    "if USE_COS:\n",
    "    full_results.to_csv(TestResults_file,header=True, index=False)\n",
    "    #\n",
    "    # Write Assistant settings out to COS\n",
    "    #\n",
    "    cos.upload_file(Filename=TestResults_path,Bucket=credentials['BUCKET'],Key=TestResults_file)\n",
    "    #\n",
    "    print (\"Kfold test results saved off to file \" + TestResults_file + \" in results area.\")\n",
    "#\n",
    "#\n",
    "if (DEBUG):\n",
    "    print (full_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2.3'></a>\n",
    "## 2.3 Build Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build lists for confusion matrix\n",
    "#\n",
    "AllTestClasses = full_results['top_intent'].to_list()\n",
    "AllPredictClasses = full_results['correct_intent'].to_list()\n",
    "#\n",
    "# Build confusion matrix feed of actual and predicted intents\n",
    "#\n",
    "confMatrix_lists = {'Actual':AllTestClasses, 'Predicted':AllPredictClasses}\n",
    "confMatrix_df = pd.DataFrame(confMatrix_lists, columns=['Actual','Predicted'])\n",
    "#\n",
    "# Build it into a data frame that you can feed to a heatmap\n",
    "#\n",
    "confusion_matrix = pd.crosstab(confMatrix_df['Actual'], confMatrix_df['Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "#\n",
    "# Print the heatmap\n",
    "#\n",
    "display(Markdown(\"### Confusion Matrix Heatmap\"))\n",
    "sn.heatmap(confusion_matrix, cmap=\"YlGnBu\", annot=True)\n",
    "#\n",
    "# If COS is enabled, save your Confusion Matrix off\n",
    "#\n",
    "if USE_COS:\n",
    "    confusion_matrix.to_csv(ConfMatrix_file,header=True, index=True)\n",
    "    #\n",
    "    # Write Assistant settings out to COS\n",
    "    #\n",
    "    cos.upload_file(Filename=ConfMatrix_path,Bucket=credentials['BUCKET'],Key=ConfMatrix_file)\n",
    "    #\n",
    "    print (\"Confusion Matrix saved off to file \" + ConfMatrix_file + \" in results area.\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2.4'></a>\n",
    "## 2.4 - Analyze the Errors\n",
    "\n",
    "This section gives you an overview of the errors made by the intent classifier on the test set.  \n",
    "\n",
    "**Note**: `System Out of Domain` labels are assigned to user examples which get classified with confidence scores less than 0.2 as Watson Assistant considers them to be irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(inferencer)\n",
    "wrongs_df = inferencer.calculate_mistakes(full_results)\n",
    "display(Markdown(\"### Intent Detection Mistakes\"))\n",
    "display(Markdown(\"Number of Test Errors: {}\".format(len(wrongs_df))))\n",
    "\n",
    "with pd.option_context('max_colwidth', 250):\n",
    "    if not wrongs_df.empty:\n",
    "        display(wrongs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3'></a>\n",
    "# Part 3: Advanced Analysis\n",
    "3.1 [Perform Analysis Using Confidence Thresholds](#part3.1)<br>\n",
    "3.2 [Analysis Interpretation at Confidence Level T](#part3.2)<br>\n",
    "3.3 [Analyze Abnormal Confidence Levels](#part3.3)<br>\n",
    "3.4 [Perform an Analysis Using Correlated Entities per Intent](#part3.4)<br>\n",
    "3.5 [Calculate Base Chatbot Scores](#part3.5)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3.1'></a>\n",
    "## 3.1 Perform Analysis Using Confidence Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(confidence_analyzer)\n",
    "# Grab a subset of the full results - ignore entities\n",
    "results = full_results[['correct_intent', 'top_confidence','top_intent','utterance']]\n",
    "analysis_df= confidence_analyzer.analysis(results,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3.2'></a>\n",
    "## 3.2 Analysis Interpretation at Confidence Level T\n",
    "\n",
    "If a certain confidence threshold T is selected, then: \n",
    "- The on-topic accuracy for test examples which cross the threshold is ***TOA***\n",
    "- The percentage of total test examples which returns confidences higher than the threshold is measured as ***Bot Coverage %***\n",
    "- If out of domain examples exist, falsely accept out of domain examples as on topic examples at a rate measured by ***FAR*** (False Acceptance Rate)Perform Analysis Using Confidence Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.index = np.arange(1, len(analysis_df)+1)\n",
    "display(analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the threshold value\n",
    "\n",
    "By selecting a higher threshold, you can potentially bias your systems so that they are more accurate in terms of determining whether an utterance is on topic or out of domain. The default confidence threshold for Watson Assistance is 0.2.  \n",
    "\n",
    "**Effect on accuracy**: When you select a higher threshold T, this can result in higher accuracy (TOA) because only examples with confidences greater than the threshold T are included.\n",
    "\n",
    "**Effect on bot coverage %**: However, when you select a higher threshold T, this can also result in the virtual assistant responding to less examples.\n",
    "\n",
    "**Deflection to human agent**: In the scenarios where the virtual assistant is setup to hand off to a human agent when it is less confident, having a higher threshold T can:  \n",
    "\n",
    "- Improve end user experience when interacting with a virtual assistant, as it continues interaction only when its highly confident\n",
    "- Result in higher costs to the customer as this can result in more deflections to the human agents \n",
    "\n",
    "Thus, there is a trade-off and you need to decide on a threshold value on a per customer basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the threshold selection on individual intents\n",
    "This section allows the examination of thresholds on specific intents.\n",
    "\n",
    "- Use `INTENT_LIST = []` to get analysis which averages across all intents\n",
    "- Use `INTENT_LIST = ['intent1', 'intent2']` to examine specific intents and threshold analysis on these intents\n",
    "- Use `INTENT_LIST = ['ALL_INTENTS']` to examine all intents and threshold analysis for each\n",
    "- Use `INTENT_LIST = [MOST_FREQUENT_INTENT]` to get analysis on the intent with the most test examples (DEFAULT)\n",
    "\n",
    "**False Acceptance Rate (FAR) for specific intents**  \n",
    "When we calculate FAR across all intents (as in previous section) we calculate fraction of out of domain examples falsely considered on topic. When we calculate FAR for specific intents, we calculate the fraction of examples which were falsely predicted to be that specific intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(confidence_analyzer)\n",
    "\n",
    "# Calculate intent with most test examples\n",
    "#for label in list(test_df['intent'].value_counts().index):\n",
    "#    if label != skills_util.OFFTOPIC_LABEL:\n",
    "#        MOST_FREQUENT_INTENT = label \n",
    "#        break\n",
    "tox_df = pd.DataFrame(workspace_data)\n",
    "#\n",
    "# This counts the intents and the number of times they occur\n",
    "#\n",
    "label_frequency = Counter(tox_df[\"intent\"]).most_common()\n",
    "frequencies = list(reversed(label_frequency))\n",
    "#\n",
    "# Pull out the most common and least common intents, and their utterance counts\n",
    "#\n",
    "min_class, min_class_len = frequencies[0]\n",
    "max_class, max_class_len = frequencies[-1]\n",
    "#\n",
    "MOST_FREQUENT_INTENT = max_class \n",
    "#\n",
    "# Specify intents of interest for analysis      \n",
    "#\n",
    "INTENT_LIST = [MOST_FREQUENT_INTENT]  \n",
    "#\n",
    "analysis_df_list = confidence_analyzer.analysis(results, INTENT_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3.3'></a>\n",
    "## 3.3 Analyze Abnormal Confidence Levels\n",
    "Every test utterance is classified as a specific intent with a specific confidence by the Watson Assistant intent classifier. It is expected that model would be confident when it correctly predicts examples and not highly confident when it incorrectly predicts examples. \n",
    "\n",
    "But this is not always true. This can be because there are anomalies in the design. Examples that are predicted correctly with low confidence and the examples that are predicted incorrectly with high confidence are cases which need to be reviewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(confidence_analyzer)\n",
    "correct_thresh, wrong_thresh = 0.3, 0.7\n",
    "correct_with_low_conf_list, incorrect_with_high_conf_list = confidence_analyzer.abnormal_conf(\n",
    "    full_results, correct_thresh, wrong_thresh)\n",
    "#\n",
    "if len(correct_with_low_conf_list) > 0:\n",
    "    display(Markdown(\"#### Examples correctedly predicted with low confidence\"))\n",
    "    with pd.option_context('max_colwidth', 250):\n",
    "        display(HTML(correct_with_low_conf_list.to_html(index=False)))\n",
    "#\n",
    "if len(incorrect_with_high_conf_list) > 0:\n",
    "    display(Markdown(\"#### Examples incorrectedly predicted with high confidence\"))\n",
    "    with pd.option_context('max_colwidth', 250):\n",
    "        display(HTML(incorrect_with_high_conf_list.to_html(index=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions to take when you have examples of abnormal confidence\n",
    "\n",
    "If there are examples which are incorrectly classified with high confidence for specific intents, it may indicate an issue in the design of those specific intents because the user examples provided for that intent may be overlapping with the design of other intents.\n",
    "\n",
    "If intent A seems to always get misclassified as intent B with high confidence or gets correctly predicted with low confidence, consider using intent conflict detection. For more information, refer to the <a href=\"https://cloud.ibm.com/docs/services/assistant?topic=assistant-intents#intents-resolve-conflicts\" target=\"_blank\" rel=\"noopener no referrer\">Conflict Resolution Documentation</a>.\n",
    "\n",
    "Also consider whether those two intents need to be two separate intents or whether they need to be merged. If they can't be merged, then consider adding more user examples which distinguish intent A specifically from intent B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3.4'></a>\n",
    "## 3.4 Perform an Analysis Using Correlated Entities per Intent\n",
    "\n",
    "Perform a chi square significance test for entities such as we or you for unigrams and bigrams in the previous section. For each utterance in the training data, this analysis will call the message API for entity detection on each utterance and find the most correlated entities for each intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(entity_analyzer)\n",
    "if entities_list:\n",
    "    entity_label_correlation_df = entity_analyzer.entity_label_correlation_analysis(\n",
    "        full_results, entities_list)\n",
    "    with pd.option_context('display.max_colwidth', 200):\n",
    "        entity_label_correlation_df.index = np.arange(1, len(entity_label_correlation_df) + 1)\n",
    "        display(entity_label_correlation_df)\n",
    "else:\n",
    "    display(Markdown(\"### Target workspace has no entities.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3.5'></a>\n",
    "## 3.5 Calculate Base Chatbot Scores\n",
    "\n",
    "Calculate the Accuracy, precision, Recall, and F1-Scores across all intents in this Watson Assistant instance, using the results from your K-Fold testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#DEBUG = True\n",
    "DEBUG = False\n",
    "#\n",
    "# Check your testing results\n",
    "#\n",
    "intent_list = confMatrix_df.Actual.unique()\n",
    "#\n",
    "# Build dicts based on list of intents\n",
    "#\n",
    "true_positives = {}\n",
    "true_negatives = {}\n",
    "false_positives = {}\n",
    "false_negatives = {}\n",
    "for intent in intent_list:\n",
    "    true_positives[intent] = 0\n",
    "    true_negatives[intent] = 0\n",
    "    false_positives[intent] = 0\n",
    "    false_negatives[intent] = 0\n",
    "#\n",
    "# Find all true positives (TP) for each intent\n",
    "#\n",
    "for (index,row) in confMatrix_df.iterrows():\n",
    "    #\n",
    "    # If Actual = Predicted, then the Actual intent has a TP,\n",
    "    # and all other intents have a TN\n",
    "    #\n",
    "    if (DEBUG):\n",
    "        print (\"Inputs ->\" + row['Actual'] + \" , \" + row['Predicted'])\n",
    "    if row['Actual'] == row['Predicted']:\n",
    "        for intent in intent_list:\n",
    "            if row['Actual'] != intent:\n",
    "                if (DEBUG):\n",
    "                    print (\"  \" + intent + \" add TN\")\n",
    "                true_negatives[intent] = true_negatives[intent] + 1\n",
    "            else:\n",
    "                if (DEBUG):\n",
    "                    print (\"  \" + intent + \" add TP\")\n",
    "                true_positives[intent] = true_positives[intent] + 1\n",
    "    if row['Actual'] != row['Predicted']:\n",
    "        for intent in intent_list:\n",
    "            if (row['Actual'] != intent) and (row['Predicted'] == intent):\n",
    "                if (DEBUG):\n",
    "                    print (\"  \" + intent + \" add FP\")\n",
    "                false_positives[intent] = false_positives[intent] + 1\n",
    "            if (row['Actual'] == intent) and (row['Predicted'] != intent):\n",
    "                if (DEBUG):\n",
    "                    print (\"  \" + intent + \" add FN\")\n",
    "                false_negatives[intent] = false_negatives[intent] + 1                \n",
    "            if (row['Actual'] != intent) and (row['Predicted'] != intent):\n",
    "                if (DEBUG):\n",
    "                    print (\"  \" + intent + \" add TN\")\n",
    "                true_negatives[intent] = true_negatives[intent] + 1\n",
    "#    \n",
    "# Calculate accuracy scores\n",
    "#\n",
    "myAccuracy = {}\n",
    "for intent in intent_list:\n",
    "    myAccuracy[intent] = (true_positives[intent] + true_negatives[intent]) / (true_positives[intent] + true_negatives[intent] + false_positives[intent] + false_negatives[intent])\n",
    "#\n",
    "# Precision is   (TP) / (TP+FP)\n",
    "#\n",
    "myPrecision = {}\n",
    "for intent in intent_list:\n",
    "    myPrecision[intent] = true_positives[intent]  / (true_positives[intent] + false_positives[intent] )\n",
    "#\n",
    "# Recall is   TP / (TP + FN)\n",
    "#\n",
    "myRecall = {}\n",
    "for intent in intent_list:\n",
    "    myRecall[intent] = true_positives[intent]  / (true_positives[intent] + false_negatives[intent] )\n",
    "#\n",
    "# F1 Score is      2 * (Precision * Recall)/ (Precision + Recall)\n",
    "#\n",
    "myF1Scores = {}\n",
    "for intent in intent_list:\n",
    "    if ((myPrecision[intent] + myRecall[intent])==0):\n",
    "        myF1Scores[intent] = \"NA\"\n",
    "    else:\n",
    "        myF1Scores[intent] = (2 * (myPrecision[intent] * myRecall[intent])) / (myPrecision[intent] + myRecall[intent])\n",
    "#\n",
    "# Dump table of scores for each intent\n",
    "#\n",
    "scoreTable = []\n",
    "for intent in intent_list:\n",
    "    tempAccuracy = (int(myAccuracy[intent] * 10000) ) / 100\n",
    "    tempPrecision = (int(myPrecision[intent] * 10000) ) / 100\n",
    "    tempRecall = (int(myRecall[intent] * 10000) ) / 100\n",
    "    if myF1Scores[intent] == \"NA\":\n",
    "        tempF1Score = \"NA\"\n",
    "    else:\n",
    "        tempF1Score = (int(myF1Scores[intent] * 10000) ) / 100\n",
    "    scoreTable.append([tempAccuracy, tempPrecision, tempRecall, tempF1Score])\n",
    "#\n",
    "scoreTable_df = pd.DataFrame(scoreTable)\n",
    "scoreTable_df.columns = [\"Accuracy\",\"Precision\",\"Recall\",\"F1 Score\"]\n",
    "scoreTable_df.index = intent_list\n",
    "display(Markdown(\"## WATSON ASSISTANT SCORES \"))\n",
    "display(scoreTable_df)\n",
    "#\n",
    "# If COS is configured, save off your scores\n",
    "#\n",
    "if USE_COS:\n",
    "    scoreTable_df.to_csv(Scores_file,header=True, index=True)\n",
    "    #\n",
    "    # Write Assistant settings out to COS\n",
    "    #\n",
    "    cos.upload_file(Filename=Scores_path,Bucket=credentials['BUCKET'],Key=Scores_file)\n",
    "    #\n",
    "    print (\"Scoring results saved off to file \" + Scores_file + \" in results area.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4'></a>\n",
    "## Part 4: Summary\n",
    "Congratulations! You have successfully completed the Assistant analysis evaluation. <br>\n",
    "This notebook is designed to improve our dialog skill analysis in an iterative fashion. Use it to tackle one aspect of your dialog skill at a time and start over for another aspect later for continuous improvement.\n",
    "\n",
    "###  Glossary\n",
    "\n",
    "**True Positives (TP):** True Positive measures the number of correctly predicted positive values meaning that predicted class is the same as the actual class which is the target intent.\n",
    "\n",
    "**True Negatives (TN):** True Negative measures the number of correctly predicted negative values meaning that the predicted class is the same as the actual class which is not the target intent.\n",
    "\n",
    "**False Positives (FP):** False Positive measures the number of incorrectly predicted positive values meaning that the predicted class is the target intent but the actual class is not the target intent.  \n",
    "\n",
    "**False Negatives (FN):** False Negatives measures the number of incorrectly predicted negative values meaning that the predicted class is not the target intent but the actual class is the target intent. \n",
    "\n",
    "**Accuracy:** Accuracy measures the ratio of corrected predicted user examples out of all user examples.   \n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)  \n",
    "\n",
    "**Precision:** Precision measures the ratio of correctly predicted positive observations out of total predicted positive observations.   \n",
    "Precision = TP / (TP + FP)  \n",
    "\n",
    "**Recall:** Recall measures the ratio of correctly predicted positive observations out of all observations of the target intent.  \n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "**F1 Score:** F1 Score is the harmonic average of Precision and Recall.  \n",
    "F1 = 2 \\* (Precision \\* Recall)/ (Precision + Recall)\n",
    "\n",
    "For more information related to Watson Assistant, refer to the <a href=\"https://cloud.ibm.com/docs/services/assistant\" target=\"_blank\" rel=\"noopener no referrer\">Watson Assistant Documentation</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
